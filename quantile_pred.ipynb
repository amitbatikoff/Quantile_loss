{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for AAPL...\n",
      "Downloading data for MSFT...\n",
      "Downloading data for GOOGL...\n",
      "Downloading data for TSLA...\n",
      "Downloading data for AMZN...\n",
      "Downloading data for META...\n",
      "Downloading data for NVDA...\n",
      "Downloading data for BRK-B...\n",
      "Downloading data for JPM...\n",
      "Downloading data for JNJ...\n",
      "Downloading data for V...\n",
      "Downloading data for PG...\n",
      "Downloading data for UNH...\n",
      "Downloading data for HD...\n",
      "Downloading data for MA...\n",
      "Downloading data for BAC...\n",
      "Downloading data for DIS...\n",
      "Downloading data for ADBE...\n",
      "Downloading data for CRM...\n",
      "Downloading data for NFLX...\n",
      "Downloading data for CSCO...\n",
      "Downloading data for PFE...\n",
      "Downloading data for ORCL...\n",
      "Downloading data for TMO...\n",
      "Downloading data for ACN...\n",
      "Downloading data for ABT...\n",
      "Downloading data for COST...\n",
      "Downloading data for PEP...\n",
      "Downloading data for AVGO...\n",
      "Downloading data for MRK...\n",
      "Downloading data for WMT...\n",
      "Downloading data for KO...\n",
      "Downloading data for VZ...\n",
      "Downloading data for CMCSA...\n",
      "Downloading data for INTC...\n",
      "Downloading data for T...\n",
      "Downloading data for CVX...\n",
      "Downloading data for DHR...\n",
      "Downloading data for QCOM...\n",
      "Downloading data for NEE...\n",
      "Downloading data for LLY...\n",
      "Downloading data for UNP...\n",
      "Downloading data for TXN...\n",
      "Downloading data for BMY...\n",
      "Downloading data for LOW...\n",
      "Downloading data for INTU...\n",
      "Downloading data for IBM...\n",
      "Downloading data for AMD...\n",
      "Downloading data for RTX...\n",
      "Downloading data for HON...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\amitb\\Python\\playground\\checkpoints exists and is not empty.\n",
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model   | Sequential   | 666 K  | train\n",
      "1 | loss_fn | QuantileLoss | 0      | train\n",
      "-------------------------------------------------\n",
      "666 K     Trainable params\n",
      "0         Non-trainable params\n",
      "666 K     Total params\n",
      "2.664     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training unified model for all stocks\n",
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 74.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 5/5 [00:00<00:00,  6.09it/s, v_num=19, train_loss=0.534, val_loss=0.865]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alpha Vantage API settings\n",
    "API_KEY = \"FGU3D4XLZIJ9Q8YJ\"\n",
    "BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "def download_stock_data(symbol, interval=\"1min\"):\n",
    "    url = f\"{BASE_URL}?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}&apikey={API_KEY}&datatype=csv&extended_hours=false&outputsize=full\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "            if df.empty:\n",
    "                raise ValueError(f\"Empty data received for {symbol}\")\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            return df\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded for {symbol}. Waiting 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            return download_stock_data(symbol, interval)\n",
    "        else:\n",
    "            raise ValueError(f\"Error fetching data for {symbol}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data for {symbol}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_stock_data(symbols):\n",
    "    stock_data = {}\n",
    "    # Get a complete set of timestamps from all stocks\n",
    "    all_timestamps = set()\n",
    "    \n",
    "    # First pass: download data and collect all timestamps\n",
    "    for symbol in symbols:\n",
    "        print(f\"Downloading data for {symbol}...\")\n",
    "        df = download_stock_data(symbol)\n",
    "        if df is not None:\n",
    "            stock_data[symbol] = df\n",
    "            all_timestamps.update(df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    \n",
    "    # Convert to sorted list\n",
    "    all_timestamps = sorted(list(all_timestamps))\n",
    "    \n",
    "    # Second pass: reindex and forward fill\n",
    "    for symbol in stock_data:\n",
    "        # Create a complete timestamp index\n",
    "        full_idx = pd.DatetimeIndex(all_timestamps)\n",
    "        # Reindex and forward fill\n",
    "        stock_data[symbol] = (stock_data[symbol]\n",
    "            .set_index('timestamp')\n",
    "            .reindex(full_idx)\n",
    "            .ffill()\n",
    "            .bfill()  # Using direct method calls instead of fillna(method=)\n",
    "            .reset_index()\n",
    "            .rename(columns={'index': 'timestamp'})  # Explicitly rename the index column\n",
    "        )\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "def split_data(stock_data):\n",
    "    train, val, test = {}, {}, {}\n",
    "    for symbol, df in stock_data.items():\n",
    "        # Ensure DataFrame has timestamp column\n",
    "        if 'timestamp' not in df.columns:\n",
    "            raise ValueError(f\"DataFrame for {symbol} is missing timestamp column. Columns: {df.columns}\")\n",
    "        \n",
    "        df = df.sort_values('timestamp')\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        grouped = df.groupby('date')\n",
    "        days = list(grouped.groups.keys())\n",
    "\n",
    "        train_end = int(len(days) * 0.6)\n",
    "        val_end = int(len(days) * 0.8)\n",
    "\n",
    "        train_days = days[:train_end]\n",
    "        val_days = days[train_end:val_end]\n",
    "        test_days = days[val_end:]\n",
    "\n",
    "        train[symbol] = df[df['date'].isin(train_days)]\n",
    "        val[symbol] = df[df['date'].isin(val_days)]\n",
    "        test[symbol] = df[df['date'].isin(test_days)]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data_dict, input_ratio=0.7, target_ratio=0.9):\n",
    "        # Combine all stock data into a single dataframe with a 'symbol' column\n",
    "        self.data = pd.concat([df.assign(symbol=symbol) for symbol, df in data_dict.items()])\n",
    "        self.input_ratio = input_ratio\n",
    "        self.target_ratio = target_ratio\n",
    "        numerical_cols = self.data.select_dtypes(include=np.number).columns\n",
    "        self.data[numerical_cols] = self.data[numerical_cols].astype('float64')\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.grouped = self.data.groupby([self.data['timestamp'].dt.date, 'symbol'])\n",
    "        self.date_symbols = list(self.grouped.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.date_symbols)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day, symbol = self.date_symbols[idx]\n",
    "        day_data = self.grouped.get_group((day, symbol)).sort_values('timestamp')\n",
    "\n",
    "        # Calculate lengths\n",
    "        total_len = len(day_data)\n",
    "        input_len = int(total_len * self.input_ratio)\n",
    "        target_idx = int(total_len * self.target_ratio)\n",
    "\n",
    "        # Get input and target data\n",
    "        input_data = day_data.iloc[:input_len][['close']].values.astype('float32')\n",
    "        target_data = day_data.iloc[target_idx]['close'].astype('float32')\n",
    "\n",
    "        return torch.tensor(input_data, dtype=torch.float32), torch.tensor([target_data], dtype=torch.float32)\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        errors = targets - preds\n",
    "        loss = torch.maximum((self.quantile - 1) * errors, self.quantile * errors)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class StockPredictor(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.loss_fn = QuantileLoss(quantile=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        targets = targets.view(-1, 1)  # Reshape targets to match output shape\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        targets = targets.view(-1, 1)  # Reshape targets to match output shape\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"TSLA\", \"AMZN\", \"META\", \"NVDA\", \"BRK-B\", \"JPM\", \"JNJ\", \n",
    "            \"V\", \"PG\", \"UNH\", \"HD\", \"MA\", \"BAC\", \"DIS\", \"ADBE\", \"CRM\", \"NFLX\",\n",
    "            \"CSCO\", \"PFE\", \"ORCL\", \"TMO\", \"ACN\", \"ABT\", \"COST\", \"PEP\", \"AVGO\", \"MRK\",\n",
    "            \"WMT\", \"KO\", \"VZ\", \"CMCSA\", \"INTC\", \"T\", \"CVX\", \"DHR\", \"QCOM\", \"NEE\",\n",
    "            \"LLY\", \"UNP\", \"TXN\", \"BMY\", \"LOW\", \"INTU\", \"IBM\", \"AMD\", \"RTX\", \"HON\"]\n",
    "stock_data = get_stock_data(symbols)\n",
    "train, val, test = split_data(stock_data)\n",
    "\n",
    "stock_split = {\n",
    "    'train': train,\n",
    "    'val': val,\n",
    "    'test': test\n",
    "}\n",
    "\n",
    "# Create dataloaders (no need for seq_length parameter anymore)\n",
    "train_loader = DataLoader(StockDataset(train), batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(StockDataset(val), batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(StockDataset(test), batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get input size from a sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "input_size = sample_batch[0].shape[1]\n",
    "\n",
    "model = StockPredictor(input_size=input_size, hidden_dim=512)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "monitor='val_loss',\n",
    "dirpath='checkpoints',\n",
    "filename='stock-predictor-{epoch:02d}-{val_loss:.2f}',\n",
    "save_top_k=3,\n",
    "mode='min'\n",
    ")\n",
    "\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "monitor='val_loss',\n",
    "patience=20,\n",
    "mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "max_epochs=1000,\n",
    "callbacks=[checkpoint_callback, early_stop_callback],\n",
    "log_every_n_steps=1,\n",
    "deterministic=True\n",
    ")\n",
    "\n",
    "# Train single model on all stocks\n",
    "print(\"\\nTraining unified model for all stocks\")\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Save unified model\n",
    "torch.save(model.state_dict(), 'unified_stock_model.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get one batch from test loader\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m test_inputs, test_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_loader\u001b[49m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Get prediction for this batch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get one batch from test loader\n",
    "test_inputs, test_targets = next(iter(test_loader))\n",
    "\n",
    "# Get prediction for this batch\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_inputs)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "sample_idx = 0  # Take the first example from the batch\n",
    "input_data = test_inputs[sample_idx].numpy()\n",
    "target = test_targets[sample_idx].numpy()\n",
    "prediction = predictions[sample_idx].numpy()\n",
    "\n",
    "# Create time points for x-axis\n",
    "time_points = np.arange(len(input_data))\n",
    "prediction_time = len(input_data)  # The prediction is made for this time point\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_points, input_data, 'b-', label='Input Data')\n",
    "plt.scatter(prediction_time, target, color='g', label='Actual Target', s=100)\n",
    "plt.scatter(prediction_time, prediction, color='r', label='Prediction', s=100)\n",
    "plt.axvline(x=prediction_time, color='gray', linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.title('Stock Price Prediction Example')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Price')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
