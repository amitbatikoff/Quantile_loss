{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data for AAPL...\n",
      "Downloading data for MSFT...\n",
      "Downloading data for GOOGL...\n",
      "Downloading data for TSLA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\amitb\\Python\\playground\\checkpoints exists and is not empty.\n",
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | model   | Sequential   | 936 K  | train\n",
      "1 | loss_fn | QuantileLoss | 0      | train\n",
      "-------------------------------------------------\n",
      "936 K     Trainable params\n",
      "0         Non-trainable params\n",
      "936 K     Total params\n",
      "3.748     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training unified model for all stocks\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\amitb\\Python\\playground\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s, v_num=9, train_loss=5.810, val_loss=7.150]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Alpha Vantage API settings\n",
    "API_KEY = \"FGU3D4XLZIJ9Q8YJ\"\n",
    "BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "def download_stock_data(symbol, interval=\"1min\"):\n",
    "    url = f\"{BASE_URL}?function=TIME_SERIES_INTRADAY&symbol={symbol}&interval={interval}&apikey={API_KEY}&datatype=csv&extended_hours=false&outputsize=full\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            df = pd.read_csv(StringIO(response.text))\n",
    "            if df.empty:\n",
    "                raise ValueError(f\"Empty data received for {symbol}\")\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            return df\n",
    "        elif response.status_code == 429:\n",
    "            print(f\"Rate limit exceeded for {symbol}. Waiting 60 seconds...\")\n",
    "            time.sleep(60)\n",
    "            return download_stock_data(symbol, interval)\n",
    "        else:\n",
    "            raise ValueError(f\"Error fetching data for {symbol}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading data for {symbol}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_stock_data(symbols):\n",
    "    stock_data = {}\n",
    "    for symbol in symbols:\n",
    "        print(f\"Downloading data for {symbol}...\")\n",
    "        stock_data[symbol] = download_stock_data(symbol)\n",
    "    return stock_data\n",
    "\n",
    "def split_data(stock_data):\n",
    "    train, val, test = {}, {}, {}\n",
    "    for symbol, df in stock_data.items():\n",
    "        df = df.sort_values('timestamp')\n",
    "        df['date'] = df['timestamp'].dt.date\n",
    "        grouped = df.groupby('date')\n",
    "        days = list(grouped.groups.keys())\n",
    "\n",
    "        train_end = int(len(days) * 0.6)\n",
    "        val_end = int(len(days) * 0.8)\n",
    "\n",
    "        train_days = days[:train_end]\n",
    "        val_days = days[train_end:val_end]\n",
    "        test_days = days[val_end:]\n",
    "\n",
    "        train[symbol] = df[df['date'].isin(train_days)]\n",
    "        val[symbol] = df[df['date'].isin(val_days)]\n",
    "        test[symbol] = df[df['date'].isin(test_days)]\n",
    "\n",
    "    return train, val, test\n",
    "\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data_dict, input_ratio=0.7, target_ratio=0.9):\n",
    "        # Combine all stock data into a single dataframe with a 'symbol' column\n",
    "        self.data = pd.concat([df.assign(symbol=symbol) for symbol, df in data_dict.items()])\n",
    "        self.input_ratio = input_ratio\n",
    "        self.target_ratio = target_ratio\n",
    "        numerical_cols = self.data.select_dtypes(include=np.number).columns\n",
    "        self.data[numerical_cols] = self.data[numerical_cols].astype('float64')\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.grouped = self.data.groupby([self.data['timestamp'].dt.date, 'symbol'])\n",
    "        self.date_symbols = list(self.grouped.groups.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.date_symbols)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        day, symbol = self.date_symbols[idx]\n",
    "        day_data = self.grouped.get_group((day, symbol)).sort_values('timestamp')\n",
    "\n",
    "        input_len = int(len(day_data) * self.input_ratio)\n",
    "        target_idx = int(len(day_data) * self.target_ratio)\n",
    "\n",
    "        input_data = day_data.iloc[:input_len][['close']].values.astype('float32')\n",
    "        target_data = day_data.iloc[target_idx]['close'].astype('float32')\n",
    "\n",
    "        return torch.tensor(input_data, dtype=torch.float32), torch.tensor([target_data], dtype=torch.float32)\n",
    "\n",
    "class QuantileLoss(nn.Module):\n",
    "    def __init__(self, quantile):\n",
    "        super(QuantileLoss, self).__init__()\n",
    "        self.quantile = quantile\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        errors = targets - preds\n",
    "        loss = torch.maximum((self.quantile - 1) * errors, self.quantile * errors)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class StockPredictor(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(StockPredictor, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.loss_fn = QuantileLoss(quantile=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        targets = targets.view(-1, 1)  # Reshape targets to match output shape\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs)\n",
    "        targets = targets.view(-1, 1)  # Reshape targets to match output shape\n",
    "        loss = self.loss_fn(outputs, targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\"\n",
    "        }\n",
    "\n",
    "def create_dataloader(stock_split, batch_size=128):\n",
    "    train_loaders = {symbol: DataLoader(StockDataset(df), batch_size=batch_size, shuffle=True)\n",
    "                     for symbol, df in stock_split['train'].items()}\n",
    "    val_loaders = {symbol: DataLoader(StockDataset(df), batch_size=batch_size, shuffle=False)\n",
    "                   for symbol, df in stock_split['val'].items()}\n",
    "    test_loaders = {symbol: DataLoader(StockDataset(df), batch_size=batch_size, shuffle=False)\n",
    "                    for symbol, df in stock_split['test'].items()}\n",
    "\n",
    "    return train_loaders, val_loaders, test_loaders\n",
    "\n",
    "def main():\n",
    "    symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"TSLA\",\"AMZN\",\"FB\",\"NVDA\",\"PYPL\",\n",
    "               \"INTC\",\"ADBE\",\"CSCO\",\"NFLX\",\"CMCSA\",\"PEP\",\"COST\",\"AMGN\",\"AVGO\",\n",
    "               \"TXN\",\"QCOM\",\"GILD\",\"SBUX\",\"INTU\",\"BKNG\",\"AMD\",\"MU\",\"ADP\",\n",
    "               \"ISRG\",\"FISV\",\"CSX\",\"VRTX\",\"REGN\",\"ILMN\"]\n",
    "    stock_data = get_stock_data(symbols)\n",
    "    train, val, test = split_data(stock_data)\n",
    "\n",
    "    stock_split = {\n",
    "        'train': train,\n",
    "        'val': val,\n",
    "        'test': test\n",
    "    }\n",
    "\n",
    "    # Create single dataloader for all stocks\n",
    "    train_loader = DataLoader(StockDataset(train), batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(StockDataset(val), batch_size=128, shuffle=False)\n",
    "    test_loader = DataLoader(StockDataset(test), batch_size=128, shuffle=False)\n",
    "\n",
    "    # Get input size from a sample batch\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    input_size = sample_batch[0].shape[1]\n",
    "\n",
    "    model = StockPredictor(input_size=input_size, hidden_dim=512)\n",
    "    \n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='checkpoints',\n",
    "        filename='stock-predictor-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    early_stop_callback = pl.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        mode='min'\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=1000,\n",
    "        callbacks=[checkpoint_callback, early_stop_callback],\n",
    "        log_every_n_steps=1,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    # Train single model on all stocks\n",
    "    print(\"\\nTraining unified model for all stocks\")\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # Save unified model\n",
    "    torch.save(model.state_dict(), 'unified_stock_model.pt')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
